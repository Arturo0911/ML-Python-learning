# -*- coding: utf-8 -*-
"""machinelearning2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-5huv4XvESCNmPLcz5b-Th5k8XEl7RJg
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

"""Regresión lineal"""

car_prices = [5,6,7,8,9,10]
units_solds = [8.5,8,7.5,7.0,6.5,6.0]

plt.scatter(car_prices, units_solds)

# constructor de array de numpy
prices_array = np.array(car_prices).reshape(-1,1)
units_array = np.array(units_solds).reshape(-1,1)
prices_array, units_array

# ahora que ya tenemos el arrego de numpy, vamos a transformarlo a 
# tensor

prices = torch.from_numpy(prices_array).float().requires_grad_(True)
# El arreglo units no necesita gradientes porque será el target
units = torch.from_numpy(units_array).float()
prices, prices.shape

model = nn.Linear(1,1)

loss_function = nn.MSELoss()

# Stochastic gradient descent or SGD
# envio cuales sonlos parametros, los pesos a partir del modelo
# y debo especificar cual es el learning rate,son los pasos que doy
# para minimizar el lost
optimizer = optim.SGD(model.parameters(), lr=0.015)
losses = []
interations = 2000

# training lopp
# por cada una de ellas haré una iteracion
# y por cada una de ellas haremos una prediccion
for i in range(interations):
  pred = model(prices)

  # la prediccion del primer paso cuanto estoy adivinando y cuando es en realidad
  loss = loss_function(pred, units)
  losses.append(loss.data)


  # backward pass como pytorch acumula, no encontraré el valor que 
  # deseo porque los gradiente se siguen acumulando
  optimizer.zero_grad()
  loss.backward()
  optimizer.step()

print(float(loss))

plt.plot(range(interations), losses)

# vamos a predecir

x = torch.tensor([[14.0]])
p = model(x)
p